{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:57:30.478664Z",
     "start_time": "2020-07-25T11:57:26.050236Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## исходные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T18:04:47.806664Z",
     "start_time": "2020-07-20T18:04:18.252510Z"
    }
   },
   "outputs": [],
   "source": [
    "train_avito_data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:57:31.153353Z",
     "start_time": "2020-07-25T11:57:30.483409Z"
    }
   },
   "outputs": [],
   "source": [
    "val_avito_data = pd.read_csv(\"data/val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## сэмплирование train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T10:51:30.998965Z",
     "start_time": "2020-07-22T10:49:01.553233Z"
    }
   },
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X_train = train_avito_data.drop(\"is_bad\", axis=1).copy()\n",
    "y_train = train_avito_data.is_bad.copy()\n",
    "\n",
    "X_res, y_res = sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T10:52:58.437045Z",
     "start_time": "2020-07-22T10:52:53.797504Z"
    }
   },
   "outputs": [],
   "source": [
    "train_avito_data_res = X_res.copy()\n",
    "train_avito_data_res[\"is_bad\"] = y_res.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "1) выделим параметры `phone_number_NOT_given_with_digits`, `contains_phone_number`, `contains_link`, `contains_vk`, `contains_fb`, `contains_instagram`. Затем сохраним те, которые будут соответствовать фродовому случаю.\n",
    "\n",
    "2) По-моему мнению, в контексте авито, чтобы не портить опыт пользователей продавцов (которые приносят основный доход) в нашем случае лучше недоловить фродстеров (мошенников), чем переловить даже неважные кейсы. Другими словами помечать только те кейсы, где есть явный паттерн нарушения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- добавил вайбер `contains_viber`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:40:47.878322Z",
     "start_time": "2020-07-22T14:40:45.717808Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# для подсчета цифр в тексте\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def contains_contact_info_minimal(df):\n",
    "\n",
    "    df[\"description_title\"] = df[\"description\"] + \" \" + df[\"title\"]\n",
    "    \n",
    "    # слова-цифры\n",
    "    words_numbers = [\"ноль\",\"один\",\"два\", \"три\", \"четыре\", \"пять\", \"шесть\", \"семь\",\"девять\", \"десять\",\n",
    "                    \"одиннадцать\", \"двенадцать\", \"тринадцать\", \"четырнадцать\", \"пятнадцать\", \"шесть\"\n",
    "                    \"шестнадцать\",\"семнадцать\",\"восемнадцать\",\"девятнадцать\",\"двадцать\",\"тридцать\",\n",
    "                    \"сорок\",\"пятьдесят\",\"шестьдесят\",\"семьдесят\",\"восемьдесят\",\"девяносто\",\n",
    "                    \"сто\",\"двести\",\"триста\",\"четыреста\",\"пятьсот\",\"шестьсот\",\"семьсот\",\"восемьсот\",\"девятьсот\"]\n",
    "    \n",
    "    df[\"contains_word_number\"] = df[\"description_title\"].apply(lambda x: \\\n",
    "                                    any(word_number in x for word_number in words_numbers))\n",
    "        \n",
    "    df[\"contains_link\"] = df[\"description_title\"].str.contains(\"https://\") |\\\n",
    "                                df[\"description_title\"].str.contains(\".ru\")\n",
    "\n",
    "    df[\"contains_vk\"] = df[\"description_title\"].str.contains(\"vk\") | \\\n",
    "                                df[\"description_title\"].str.contains(\"вконтакте\") | \\\n",
    "                                df[\"description_title\"].str.contains(\"в контакте\")\n",
    "\n",
    "    df[\"contains_inst\"] = df[\"description_title\"].str.contains(\"instagram\") | \\\n",
    "                                df[\"description_title\"].str.contains(\"инстагр\")\n",
    "\n",
    "    df[\"contains_fb\"] = df[\"description_title\"].str.contains(\"facebook\") | \\\n",
    "                                df[\"description_title\"].str.contains(\"фейсбук\")\n",
    "    \n",
    "    df[\"contains_mail\"] = df[\"description_title\"].str.contains(\"mail\") | \\\n",
    "                                df[\"description_title\"].str.contains(\"@\")\n",
    "    \n",
    "    df[\"contains_viber\"] = df[\"description_title\"].str.contains(\"viber\") | \\\n",
    "                                df[\"description_title\"].str.contains(\"вайбер\")\n",
    "    \n",
    "    df[\"contains_whatsapp\"] = df[\"description_title\"].str.contains(\"whatsapp\") | \\\n",
    "                                df[\"description_title\"].str.contains(\"ватсап\")\n",
    "    \n",
    "    #с наиб. вероятностью содержится номер телефона\n",
    "    df[\"contains_phone_number\"] = df[\"description_title\"]\\\n",
    "                                    .str.contains(\"^((8|\\+7)[\\- ]?)?(\\(?\\d{3}\\)?[\\- ]?)?[\\d\\- ]{7,10}$\")\n",
    "    \n",
    "    # еще одна проверка на наличие телефона\n",
    "    df[\"description_numbers\"] = df[\"description\"].apply(lambda x: \"\".join(re.findall(r'(\\d+)', x)))\n",
    "    \n",
    "    df[\"contains_phone_number_in_extracted_numbers\"] = df[\"description_numbers\"]\\\n",
    "                                    .str.contains(\"^((8|\\+7)[\\- ]?)?(\\(?\\d{3}\\)?[\\- ]?)?[\\d\\- ]{7,10}$\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def count_digits(string):\n",
    "    return sum(item.isdigit() for item in string)\n",
    "\n",
    "\n",
    "def prepare_data(train_data=None, val_data=None):\n",
    "    \n",
    "    \n",
    "    if train_data is not None and val_data is None:\n",
    "        df = train_data.copy()\n",
    "    if val_data is not None and train_data is not None:\n",
    "        df = val_data.copy()\n",
    "    \n",
    "    #проведем лематизацию title и description\n",
    "    df[\"description_lem\"] = df[\"description\"].apply(lambda x: preprocess_text(x))\n",
    "    df[\"title_lem\"] = df[\"title\"].apply(lambda x: preprocess_text(x))\n",
    "    \n",
    "    #кол-во пересечений по словам в сырой и лем версии\n",
    "    # гипотеза - цель фродстера пропиариться в описании, а не продать что-то, поэтому пересечений в его случае\n",
    "    # должно быть меньше\n",
    "    df[\"intersection_title_desc\"] = [len(set(a).intersection(b)) for a, b in zip(df.title, df.description)]\n",
    "    df[\"intersection_title_desc_lem\"] = [len(set(a).intersection(b)) for a, b \\\n",
    "                                         in zip(df.title_lem, df.description_lem)]\n",
    "    \n",
    "\n",
    "    \n",
    "    # улучшим разметку фрода в train\n",
    "    if train_data is not None and val_data is None:\n",
    "        \n",
    "        fraud = df[df[\"is_bad\"]==1].copy()\n",
    "        \n",
    "        fraud = contains_contact_info_minimal(fraud)\n",
    "        \n",
    "        developed_fraud = fraud.query(\"contains_word_number == True or \\\n",
    "                contains_link == True or contains_vk == True or contains_inst == True or \\\n",
    "                contains_fb == True or contains_phone_number==True or contains_mail==True or\\\n",
    "                contains_viber == True or contains_phone_number_in_extracted_numbers==True or\\\n",
    "                contains_whatsapp == True\").copy()\n",
    "\n",
    "        df.loc[df.index.isin(developed_fraud.index), \"fraud_developed\"] = 1\n",
    "        df.loc[~(df.index.isin(developed_fraud.index)), \"fraud_developed\"] = 0\n",
    "        \n",
    "        # вернем старое название\n",
    "        df = df.drop(\"is_bad\", axis=1).rename(columns={\"fraud_developed\": \"is_bad\"})\n",
    "        \n",
    "        del fraud\n",
    "        del developed_fraud\n",
    "        \n",
    "        #gc.collect()\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"step_1 completed\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # из EDA получилось выделить слова-тригеры, здесь использован эвристический способ расчета  \n",
    "    # метрики задевания слов тригеров\n",
    "    counter = Counter()\n",
    "\n",
    "    df['title_lem'].apply(lambda x: counter.update(x.split(\" \")))\n",
    "    \n",
    "    words_triggers_title = ['м²', 'эта', 'квартира','участок','сотня','продавать','дом','ваз','5','6',\n",
    "                            '4','3','2','1','iphone','lada','ижс','новый','гараж','шина','котенок',\n",
    "                            'дача','коляска','комната','платье','колесо','7','продаваться','ремонт',\n",
    "                            'рука','диван','велосипед','priora','8','газ','кровать','детский','диск',\n",
    "                            'отдавать','зимний','газель','работа','samara','студия','щенок','дверь',\n",
    "                            'детская','samsung', 'маникюр','toyota','запчасть','днп','снт','резина',\n",
    "                            'наушник','репетитор','машина', 'корова']\n",
    "    \n",
    "    df_words_triggers_title = pd.DataFrame(counter.most_common(500))\n",
    "\n",
    "    df_words_triggers_title[0] = df_words_triggers_title[0].apply(preprocess_text)\n",
    "\n",
    "    df_words_triggers_title[1] = df_words_triggers_title[1]/ 1000\n",
    "\n",
    "    df_words_triggers_title = df_words_triggers_title[df_words_triggers_title[0].str.strip()!=\"\"]\n",
    "\n",
    "    df_words_triggers_title = df_words_triggers_title.rename(columns={0:\"word\", 1:\"count\"})\\\n",
    "                                    .groupby(\"word\").apply(sum).sort_values(\"count\", ascending=False)\\\n",
    "                                    [\"count\"].reset_index()\n",
    "    \n",
    "    df_words_triggers_title = df_words_triggers_title[df_words_triggers_title[\"word\"]\\\n",
    "                                    .isin(words_triggers_title)]\n",
    "    \n",
    "    df[\"title_lem_proba_fraud\"] = df[\"title_lem\"].str.lower().apply(lambda x: df_words_triggers_title\\\n",
    "                            [df_words_triggers_title[\"word\"].isin(x.split(\" \"))][\"count\"].sum())\n",
    "    \n",
    "    \n",
    "    counter = Counter()\n",
    "\n",
    "    df['description_lem'].apply(lambda x: counter.update(x.split(\" \")))\n",
    "    \n",
    "    words_triggers_description = ['телефон', 'звонить', 'состояние', 'номер', 'вопрос', 'продавать','цена', \n",
    "                                  'квартира','хороший', '8','—\\n','торг','это','работа','год','6','5','3',\n",
    "                                  '4','1','продаваться','2','любой','дом', 'наш','7','очень','магазин',\n",
    "                                  'отличный','новый','тело','–\\n','мочь','большой','наличие','доставка',\n",
    "                                  'г','связь','обращаться','дома','россия','писать','весь','запчасть','сайт',\n",
    "                                  '↓','руб','заказ','9']\n",
    "    \n",
    "    df_words_triggers_description = pd.DataFrame(counter.most_common(500))\n",
    "\n",
    "    df_words_triggers_description[0] = df_words_triggers_description[0].apply(preprocess_text)\n",
    "\n",
    "    df_words_triggers_description[1] = df_words_triggers_description[1]/ 1000\n",
    "\n",
    "    df_words_triggers_description = df_words_triggers_description[df_words_triggers_description[0].str.strip()!=\"\"]\n",
    "\n",
    "    df_words_triggers_description = df_words_triggers_description.rename(columns={0:\"word\", 1:\"count\"})\\\n",
    "                                    .groupby(\"word\").apply(sum).sort_values(\"count\", ascending=False)\\\n",
    "                                    [\"count\"].reset_index()\n",
    "    \n",
    "    df_words_triggers_description = df_words_triggers_description[df_words_triggers_description[\"word\"]\\\n",
    "                                    .isin(words_triggers_description)]\n",
    "    \n",
    "    df[\"desc_lem_proba_fraud\"] = df[\"description_lem\"].str.lower().apply(lambda x: df_words_triggers_description\\\n",
    "                            [df_words_triggers_description[\"word\"].isin(x.split(\" \"))][\"count\"].sum())\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"step_2 completed\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # уберем явных аутлаеров в категории цены (в train), чтобы они не испортили замену на средние значения\n",
    "    if train_data is not None and val_data is None:    \n",
    "        df = df[np.abs(df.price-df.price.mean()) <= (3*df.price.std())]\n",
    "    \n",
    "    # обработка пропущенных значени в price (есть пропущенные в обоих train и val)\n",
    "    df[\"price\"] = df.loc[:,\"price\"].fillna(df.groupby(\"subcategory\")[\"price\"]\\\n",
    "                               .transform(\"mean\"))\n",
    "\n",
    "    # добавим метрику отличия от средней цены по субкатегории\n",
    "    df[\"average_price_in_subcategory\"] = df.groupby('subcategory')['price'].transform('mean')\n",
    "    \n",
    "    #поскольку метрика цены давала нам прирост точности 6%, попробуем использовать еще и прозв-ую хар-ку\n",
    "    df[\"price_diff_from_subcategory\"] = df[\"price\"] - df[\"average_price_in_subcategory\"]\n",
    "\n",
    "    # добавим вспомогательные, простые параметры текста title, description для модели\n",
    "    df[\"description_len\"] = df[\"description\"].apply(len)\n",
    "    df[\"title_len\"] = df[\"title\"].apply(len)\n",
    "    df[\"number_of_words_description\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    df[\"number_of_words_title\"] = df[\"title\"].apply(lambda x: len(x.split(\" \")))\n",
    "    df[\"number_of_sentences_description\"] = df[\"description\"].apply(lambda x: x.count(\".\")+\\\n",
    "                                                                    x.count(\"!\")+x.count(\"?\"))\n",
    "    \n",
    "    # те же самые параметры для лематизированной версии\n",
    "    df[\"lem_description_len\"] = df[\"description_lem\"].apply(len)\n",
    "    df[\"lem_title_len\"] = df[\"title_lem\"].apply(len)\n",
    "    df[\"lem_number_of_words_description\"] = df[\"description_lem\"].apply(lambda x: len(x.split(\" \")))\n",
    "    df[\"lem_number_of_words_title\"] = df[\"title_lem\"].apply(lambda x: len(x.split(\" \")))\n",
    "    df[\"lem_number_of_sentences_description\"] = df[\"description_lem\"].apply(lambda x: x.count(\".\")+\\\n",
    "                                                                    x.count(\"!\")+x.count(\"?\"))\n",
    "    \n",
    "    print(\"step_3 completed\")\n",
    "    \n",
    "    \n",
    "    df[\"description_title\"] = df[\"description\"] + \" \" + df[\"title\"]\n",
    "    \n",
    "    df[\"number_of_digits\"] = df[\"description_title\"].apply(count_digits)\n",
    "    \n",
    "    # посчитаем кол-во капса в описании + тайтле, тексте (предположение, что фродстеры более склонны к привлечению внимания)\n",
    "    df['desc_title_number_of_uppercase'] = df['description_title'].str.findall(r'[A-Z]').str.len()\n",
    "    \n",
    "    # посчитаем отношение длины символов (не буквы и цифры) ко всему тексту (значки опять же оружие фродстеров)\n",
    "    df[\"description_title_ratio_of_non_words_number\"] = 1 - df['description_title'].apply(lambda x: \\\n",
    "                                                      sum(c.isalpha() for c in \"\".join(str(x).split())) \\\n",
    "                                                    + sum(c.isdigit() for c in \"\".join(str(x).split()))) \\\n",
    "                                        / df[\"description_title\"].apply(lambda x: len(\"\".join(str(x).split())))\n",
    "    \n",
    "    # подсчитаем среднюю длину слова в title, description в исходной и лематизированной версии\n",
    "    df[\"avearge_word_len_title\"] = df[\"title\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")) \\\n",
    "                                                     / len(str(x).split(\" \")))\n",
    "    df[\"avearge_word_len_title_lem\"] = df[\"title_lem\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")) \\\n",
    "                                                             / len(str(x).split(\" \")))\n",
    "    df[\"avearge_word_len_desc\"] = df[\"description\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")) \\\n",
    "                                                          / len(str(x).split(\" \")))\n",
    "    df[\"avearge_word_len_desc_lem\"] = df[\"description_lem\"].apply(lambda x: sum(len(word) \\\n",
    "                                                            for word in str(x).split(\" \")) / len(str(x).split(\" \")))\n",
    "    \n",
    "    #label encoding для 'subcategory', 'region', 'city' (эти показатели остояли свою важность)\n",
    "    object_cols = ['subcategory', 'region', 'city']\n",
    "    \n",
    "    # важно, чтобы label encoding из train полностью соответствовал validate\n",
    "    if val_data is not None and train_data is not None:\n",
    "        label_encoder_subcategory = LabelEncoder()\n",
    "        label_encoder_region = LabelEncoder()\n",
    "        label_encoder_city = LabelEncoder()\n",
    "        \n",
    "        label_encoder_subcategory.fit(train_data[\"subcategory\"])\n",
    "        label_encoder_region.fit(train_data[\"region\"])\n",
    "        label_encoder_city.fit(train_data[\"city\"])\n",
    "        \n",
    "        df[\"subcategory\"] = label_encoder_subcategory.transform(df[\"subcategory\"])\n",
    "        df[\"region\"] = label_encoder_region.transform(df[\"region\"])\n",
    "        df[\"city\"] = label_encoder_city.transform(df[\"city\"])\n",
    "    \n",
    "    if train_data is not None and val_data is None:\n",
    "        label_encoder = LabelEncoder()\n",
    "        for col in object_cols:\n",
    "            df[col] = label_encoder.fit_transform(df[col])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Final step_4 completed, data is prepared, enjoy!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбиение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T15:54:19.089187Z",
     "start_time": "2020-07-22T14:40:47.885004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_1 completed\n",
      "step_2 completed\n",
      "Final step_4 completed, data is prepared, enjoy!\n"
     ]
    }
   ],
   "source": [
    "train_data = prepare_data(train_avito_data_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T16:48:15.046579Z",
     "start_time": "2020-07-22T16:39:38.204427Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"is_bad\", axis=1).copy()\n",
    "y_train = train_data.is_bad.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T16:52:39.383774Z",
     "start_time": "2020-07-22T16:48:17.323841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_1 completed\n",
      "step_2 completed\n",
      "Final step_4 completed, data is prepared, enjoy!\n"
     ]
    }
   ],
   "source": [
    "val_data = prepare_data(train_avito_data, val_avito_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T16:52:40.011827Z",
     "start_time": "2020-07-22T16:52:39.403340Z"
    }
   },
   "outputs": [],
   "source": [
    "X_val = val_data.drop(\"is_bad\", axis=1).copy()\n",
    "y_val = val_data.is_bad.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:23:06.675320Z",
     "start_time": "2020-07-22T16:52:40.020790Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.to_pickle(\"data/train_data_with_metrics_resampled.p\", compression=\"gzip\")\n",
    "val_data.to_pickle(\"data/val_data_with_metrics_resampled.p\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:52:21.567018Z",
     "start_time": "2020-07-22T17:52:21.493522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'description', 'subcategory', 'category', 'price', 'region',\n",
       "       'city', 'datetime_submitted', 'description_lem', 'title_lem',\n",
       "       'intersection_title_desc', 'intersection_title_desc_lem',\n",
       "       'title_lem_proba_fraud', 'desc_lem_proba_fraud',\n",
       "       'average_price_in_subcategory', 'price_diff_from_subcategory',\n",
       "       'description_len', 'title_len', 'number_of_words_description',\n",
       "       'number_of_words_title', 'number_of_sentences_description',\n",
       "       'lem_description_len', 'lem_title_len',\n",
       "       'lem_number_of_words_description', 'lem_number_of_words_title',\n",
       "       'lem_number_of_sentences_description',\n",
       "       'lem_to_original_description_len', 'lem_to_original_title_len',\n",
       "       'lem_to_original_number_of_words_description',\n",
       "       'lem_to_original_number_of_words_title',\n",
       "       'lem_to_original_sentences_description', 'description_title',\n",
       "       'number_of_digits', 'desc_title_number_of_uppercase',\n",
       "       'description_title_ratio_of_non_words_number', 'avearge_word_len_title',\n",
       "       'avearge_word_len_title_lem', 'avearge_word_len_desc',\n",
       "       'avearge_word_len_desc_lem'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# выбор данных для обучения и выбор модели классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- т.к. данные сильно несбалансированы, то стоит сделать undersampling\n",
    "- за базовую модель возьем catboost\n",
    "- найдем наилучшие параметры с помощью gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T09:59:49.699012Z",
     "start_time": "2020-07-25T09:57:04.390680Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"data/train_data_with_metrics_resampled.p\", compression=\"gzip\")\n",
    "train_data = train_data.drop_duplicates()\n",
    "\n",
    "val_data = pd.read_pickle(\"data/val_data_with_metrics_resampled.p\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:00:20.345921Z",
     "start_time": "2020-07-25T09:59:49.880446Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['title', 'description', 'category', 'datetime_submitted',\n",
    "                             \"description_lem\", \"title_lem\", \"description_title\"], axis = 1).copy()\n",
    "\n",
    "val_data = val_data.drop(['title', 'description', 'category', 'datetime_submitted',\n",
    "                             \"description_lem\", \"title_lem\", \"description_title\"], axis = 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:00:20.866903Z",
     "start_time": "2020-07-25T10:00:20.357681Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop([\"lem_to_original_sentences_description\",\n",
    "                \"lem_to_original_description_len\", \"lem_to_original_title_len\",\n",
    "                \"lem_to_original_number_of_words_description\", \"lem_to_original_number_of_words_title\"], axis=1)\n",
    "\n",
    "train_data[\"price_diff_from_subcategory\"] = abs(train_data[\"price_diff_from_subcategory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:00:20.891770Z",
     "start_time": "2020-07-25T10:00:20.873597Z"
    }
   },
   "outputs": [],
   "source": [
    "val_data = val_data.drop([\"lem_to_original_sentences_description\",\n",
    "                \"lem_to_original_description_len\", \"lem_to_original_title_len\",\n",
    "                \"lem_to_original_number_of_words_description\", \"lem_to_original_number_of_words_title\"], axis=1)\n",
    "\n",
    "val_data[\"price_diff_from_subcategory\"] = abs(val_data[\"price_diff_from_subcategory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:00:21.151262Z",
     "start_time": "2020-07-25T10:00:20.897545Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"is_bad\", axis=1).copy()\n",
    "y_train = train_data.is_bad.copy()\n",
    "\n",
    "X_val = val_data.drop(\"is_bad\", axis=1).copy()\n",
    "y_val = val_data.is_bad.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:12:12.338815Z",
     "start_time": "2020-07-23T21:12:11.343822Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Specs</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>price_diff_from_subcategory</td>\n",
       "      <td>3.021275e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>price</td>\n",
       "      <td>2.492465e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>average_price_in_subcategory</td>\n",
       "      <td>1.056404e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>description_len</td>\n",
       "      <td>1.854779e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lem_description_len</td>\n",
       "      <td>1.595202e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>number_of_digits</td>\n",
       "      <td>6.092866e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>desc_title_number_of_uppercase</td>\n",
       "      <td>5.871635e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lem_number_of_words_description</td>\n",
       "      <td>3.373265e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>title_lem_proba_fraud</td>\n",
       "      <td>2.575343e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city</td>\n",
       "      <td>2.310800e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>desc_lem_proba_fraud</td>\n",
       "      <td>2.307013e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>number_of_words_description</td>\n",
       "      <td>2.236761e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subcategory</td>\n",
       "      <td>3.519365e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>intersection_title_desc</td>\n",
       "      <td>2.557554e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>title_len</td>\n",
       "      <td>2.418022e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lem_title_len</td>\n",
       "      <td>2.323599e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lem_number_of_sentences_description</td>\n",
       "      <td>2.226615e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>intersection_title_desc_lem</td>\n",
       "      <td>2.165980e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>number_of_sentences_description</td>\n",
       "      <td>2.117577e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>region</td>\n",
       "      <td>1.247651e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lem_number_of_words_title</td>\n",
       "      <td>2.318805e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>number_of_words_title</td>\n",
       "      <td>1.036418e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>avearge_word_len_desc_lem</td>\n",
       "      <td>9.674543e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>avearge_word_len_title</td>\n",
       "      <td>7.359208e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>avearge_word_len_desc</td>\n",
       "      <td>1.693045e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>description_title_ratio_of_non_words_number</td>\n",
       "      <td>1.569933e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>avearge_word_len_title_lem</td>\n",
       "      <td>1.080872e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Specs         Score\n",
       "9                   price_diff_from_subcategory  3.021275e+10\n",
       "1                                         price  2.492465e+10\n",
       "8                  average_price_in_subcategory  1.056404e+10\n",
       "10                              description_len  1.854779e+06\n",
       "15                          lem_description_len  1.595202e+06\n",
       "20                             number_of_digits  6.092866e+05\n",
       "21               desc_title_number_of_uppercase  5.871635e+05\n",
       "17              lem_number_of_words_description  3.373265e+05\n",
       "6                         title_lem_proba_fraud  2.575343e+05\n",
       "3                                          city  2.310800e+05\n",
       "7                          desc_lem_proba_fraud  2.307013e+05\n",
       "12                  number_of_words_description  2.236761e+05\n",
       "0                                   subcategory  3.519365e+04\n",
       "4                       intersection_title_desc  2.557554e+04\n",
       "11                                    title_len  2.418022e+04\n",
       "16                                lem_title_len  2.323599e+04\n",
       "19          lem_number_of_sentences_description  2.226615e+04\n",
       "5                   intersection_title_desc_lem  2.165980e+04\n",
       "14              number_of_sentences_description  2.117577e+04\n",
       "2                                        region  1.247651e+04\n",
       "18                    lem_number_of_words_title  2.318805e+03\n",
       "13                        number_of_words_title  1.036418e+03\n",
       "26                    avearge_word_len_desc_lem  9.674543e+02\n",
       "23                       avearge_word_len_title  7.359208e+02\n",
       "25                        avearge_word_len_desc  1.693045e+02\n",
       "22  description_title_ratio_of_non_words_number  1.569933e+02\n",
       "24                   avearge_word_len_title_lem  1.080872e+02"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=27)\n",
    "\n",
    "fit = bestfeatures.fit(X_train, y_train)\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "featureScores.sort_values(\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:00:52.393399Z",
     "start_time": "2020-07-25T10:00:52.313611Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train[sorted(X_train.columns)]\n",
    "X_val = X_val[sorted(X_val.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:29:38.815511Z",
     "start_time": "2020-07-25T11:27:43.485906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=1,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для\n",
    "model_1 = XGBClassifier(random_state = 1, \n",
    "                       learning_rate=0.01)\n",
    "\n",
    "model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:29:38.924598Z",
     "start_time": "2020-07-25T11:29:38.820193Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_1 = model_1.predict_proba(X_val).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:24:46.975507Z",
     "start_time": "2020-07-25T10:24:46.966986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729876208659235"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:24:47.404531Z",
     "start_time": "2020-07-25T10:24:47.377330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12211,  3641],\n",
       "       [   45,   340]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:24:49.103034Z",
     "start_time": "2020-07-25T10:24:49.090230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8267148886944495"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:29:58.947529Z",
     "start_time": "2020-07-25T11:29:38.930832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6590800\ttotal: 220ms\tremaining: 24s\n",
      "1:\tlearn: 0.6326330\ttotal: 389ms\tremaining: 21s\n",
      "2:\tlearn: 0.6097775\ttotal: 575ms\tremaining: 20.5s\n",
      "3:\tlearn: 0.5934699\ttotal: 741ms\tremaining: 19.6s\n",
      "4:\tlearn: 0.5791101\ttotal: 936ms\tremaining: 19.7s\n",
      "5:\tlearn: 0.5686880\ttotal: 1.09s\tremaining: 18.8s\n",
      "6:\tlearn: 0.5574059\ttotal: 1.25s\tremaining: 18.5s\n",
      "7:\tlearn: 0.5480197\ttotal: 1.41s\tremaining: 18s\n",
      "8:\tlearn: 0.5400411\ttotal: 1.57s\tremaining: 17.6s\n",
      "9:\tlearn: 0.5336025\ttotal: 1.72s\tremaining: 17.2s\n",
      "10:\tlearn: 0.5283404\ttotal: 1.88s\tremaining: 16.9s\n",
      "11:\tlearn: 0.5243932\ttotal: 2.14s\tremaining: 17.5s\n",
      "12:\tlearn: 0.5200681\ttotal: 2.29s\tremaining: 17.1s\n",
      "13:\tlearn: 0.5152016\ttotal: 2.48s\tremaining: 17s\n",
      "14:\tlearn: 0.5115656\ttotal: 2.65s\tremaining: 16.8s\n",
      "15:\tlearn: 0.5088257\ttotal: 2.81s\tremaining: 16.5s\n",
      "16:\tlearn: 0.5065214\ttotal: 2.96s\tremaining: 16.2s\n",
      "17:\tlearn: 0.5040072\ttotal: 3.14s\tremaining: 16s\n",
      "18:\tlearn: 0.5018871\ttotal: 3.3s\tremaining: 15.8s\n",
      "19:\tlearn: 0.4996705\ttotal: 3.46s\tremaining: 15.6s\n",
      "20:\tlearn: 0.4979753\ttotal: 3.62s\tremaining: 15.3s\n",
      "21:\tlearn: 0.4959452\ttotal: 3.79s\tremaining: 15.1s\n",
      "22:\tlearn: 0.4944956\ttotal: 3.95s\tremaining: 15s\n",
      "23:\tlearn: 0.4920256\ttotal: 4.13s\tremaining: 14.8s\n",
      "24:\tlearn: 0.4903526\ttotal: 4.31s\tremaining: 14.6s\n",
      "25:\tlearn: 0.4890086\ttotal: 4.49s\tremaining: 14.5s\n",
      "26:\tlearn: 0.4878118\ttotal: 4.63s\tremaining: 14.2s\n",
      "27:\tlearn: 0.4865658\ttotal: 4.79s\tremaining: 14s\n",
      "28:\tlearn: 0.4853782\ttotal: 5.04s\tremaining: 14.1s\n",
      "29:\tlearn: 0.4843812\ttotal: 5.23s\tremaining: 14s\n",
      "30:\tlearn: 0.4833496\ttotal: 5.44s\tremaining: 13.9s\n",
      "31:\tlearn: 0.4815847\ttotal: 5.59s\tremaining: 13.6s\n",
      "32:\tlearn: 0.4804001\ttotal: 5.77s\tremaining: 13.5s\n",
      "33:\tlearn: 0.4792333\ttotal: 5.97s\tremaining: 13.3s\n",
      "34:\tlearn: 0.4783834\ttotal: 6.28s\tremaining: 13.5s\n",
      "35:\tlearn: 0.4768147\ttotal: 6.56s\tremaining: 13.5s\n",
      "36:\tlearn: 0.4755212\ttotal: 6.8s\tremaining: 13.4s\n",
      "37:\tlearn: 0.4744783\ttotal: 7.03s\tremaining: 13.3s\n",
      "38:\tlearn: 0.4731350\ttotal: 7.2s\tremaining: 13.1s\n",
      "39:\tlearn: 0.4722280\ttotal: 7.44s\tremaining: 13s\n",
      "40:\tlearn: 0.4713682\ttotal: 7.69s\tremaining: 12.9s\n",
      "41:\tlearn: 0.4705261\ttotal: 7.95s\tremaining: 12.9s\n",
      "42:\tlearn: 0.4696142\ttotal: 8.1s\tremaining: 12.6s\n",
      "43:\tlearn: 0.4685467\ttotal: 8.26s\tremaining: 12.4s\n",
      "44:\tlearn: 0.4675326\ttotal: 8.42s\tremaining: 12.2s\n",
      "45:\tlearn: 0.4663292\ttotal: 8.6s\tremaining: 12s\n",
      "46:\tlearn: 0.4655795\ttotal: 8.79s\tremaining: 11.8s\n",
      "47:\tlearn: 0.4649981\ttotal: 8.96s\tremaining: 11.6s\n",
      "48:\tlearn: 0.4645919\ttotal: 9.11s\tremaining: 11.3s\n",
      "49:\tlearn: 0.4638635\ttotal: 9.29s\tremaining: 11.1s\n",
      "50:\tlearn: 0.4632392\ttotal: 9.43s\tremaining: 10.9s\n",
      "51:\tlearn: 0.4624751\ttotal: 9.59s\tremaining: 10.7s\n",
      "52:\tlearn: 0.4618139\ttotal: 9.74s\tremaining: 10.5s\n",
      "53:\tlearn: 0.4612336\ttotal: 9.92s\tremaining: 10.3s\n",
      "54:\tlearn: 0.4602459\ttotal: 10.1s\tremaining: 10.1s\n",
      "55:\tlearn: 0.4598562\ttotal: 10.3s\tremaining: 9.9s\n",
      "56:\tlearn: 0.4593999\ttotal: 10.4s\tremaining: 9.69s\n",
      "57:\tlearn: 0.4588023\ttotal: 10.6s\tremaining: 9.51s\n",
      "58:\tlearn: 0.4583810\ttotal: 10.8s\tremaining: 9.32s\n",
      "59:\tlearn: 0.4577684\ttotal: 10.9s\tremaining: 9.12s\n",
      "60:\tlearn: 0.4572509\ttotal: 11.1s\tremaining: 8.92s\n",
      "61:\tlearn: 0.4568900\ttotal: 11.3s\tremaining: 8.73s\n",
      "62:\tlearn: 0.4562667\ttotal: 11.4s\tremaining: 8.51s\n",
      "63:\tlearn: 0.4558816\ttotal: 11.6s\tremaining: 8.32s\n",
      "64:\tlearn: 0.4549349\ttotal: 11.7s\tremaining: 8.12s\n",
      "65:\tlearn: 0.4545518\ttotal: 11.9s\tremaining: 7.93s\n",
      "66:\tlearn: 0.4539711\ttotal: 12.1s\tremaining: 7.75s\n",
      "67:\tlearn: 0.4535460\ttotal: 12.3s\tremaining: 7.58s\n",
      "68:\tlearn: 0.4530121\ttotal: 12.4s\tremaining: 7.38s\n",
      "69:\tlearn: 0.4526790\ttotal: 12.6s\tremaining: 7.21s\n",
      "70:\tlearn: 0.4522909\ttotal: 12.8s\tremaining: 7.01s\n",
      "71:\tlearn: 0.4518756\ttotal: 12.9s\tremaining: 6.82s\n",
      "72:\tlearn: 0.4514214\ttotal: 13.1s\tremaining: 6.63s\n",
      "73:\tlearn: 0.4510185\ttotal: 13.2s\tremaining: 6.44s\n",
      "74:\tlearn: 0.4506802\ttotal: 13.4s\tremaining: 6.25s\n",
      "75:\tlearn: 0.4502899\ttotal: 13.6s\tremaining: 6.07s\n",
      "76:\tlearn: 0.4498797\ttotal: 13.7s\tremaining: 5.87s\n",
      "77:\tlearn: 0.4491123\ttotal: 13.9s\tremaining: 5.69s\n",
      "78:\tlearn: 0.4488290\ttotal: 14s\tremaining: 5.5s\n",
      "79:\tlearn: 0.4482788\ttotal: 14.2s\tremaining: 5.33s\n",
      "80:\tlearn: 0.4478500\ttotal: 14.4s\tremaining: 5.14s\n",
      "81:\tlearn: 0.4472678\ttotal: 14.6s\tremaining: 4.97s\n",
      "82:\tlearn: 0.4470040\ttotal: 14.7s\tremaining: 4.79s\n",
      "83:\tlearn: 0.4465591\ttotal: 14.9s\tremaining: 4.6s\n",
      "84:\tlearn: 0.4461343\ttotal: 15s\tremaining: 4.41s\n",
      "85:\tlearn: 0.4458339\ttotal: 15.2s\tremaining: 4.23s\n",
      "86:\tlearn: 0.4454984\ttotal: 15.3s\tremaining: 4.06s\n",
      "87:\tlearn: 0.4452021\ttotal: 15.5s\tremaining: 3.87s\n",
      "88:\tlearn: 0.4448825\ttotal: 15.7s\tremaining: 3.69s\n",
      "89:\tlearn: 0.4446554\ttotal: 15.8s\tremaining: 3.52s\n",
      "90:\tlearn: 0.4443776\ttotal: 16s\tremaining: 3.34s\n",
      "91:\tlearn: 0.4440540\ttotal: 16.2s\tremaining: 3.17s\n",
      "92:\tlearn: 0.4436444\ttotal: 16.3s\tremaining: 2.98s\n",
      "93:\tlearn: 0.4432833\ttotal: 16.5s\tremaining: 2.81s\n",
      "94:\tlearn: 0.4430020\ttotal: 16.6s\tremaining: 2.63s\n",
      "95:\tlearn: 0.4426216\ttotal: 16.8s\tremaining: 2.45s\n",
      "96:\tlearn: 0.4424887\ttotal: 17s\tremaining: 2.28s\n",
      "97:\tlearn: 0.4420368\ttotal: 17.2s\tremaining: 2.1s\n",
      "98:\tlearn: 0.4415833\ttotal: 17.3s\tremaining: 1.93s\n",
      "99:\tlearn: 0.4410948\ttotal: 17.5s\tremaining: 1.75s\n",
      "100:\tlearn: 0.4405541\ttotal: 17.7s\tremaining: 1.57s\n",
      "101:\tlearn: 0.4401850\ttotal: 17.8s\tremaining: 1.4s\n",
      "102:\tlearn: 0.4399844\ttotal: 18s\tremaining: 1.22s\n",
      "103:\tlearn: 0.4397976\ttotal: 18.1s\tremaining: 1.05s\n",
      "104:\tlearn: 0.4392680\ttotal: 18.3s\tremaining: 871ms\n",
      "105:\tlearn: 0.4388095\ttotal: 18.4s\tremaining: 696ms\n",
      "106:\tlearn: 0.4385308\ttotal: 18.6s\tremaining: 522ms\n",
      "107:\tlearn: 0.4382297\ttotal: 18.8s\tremaining: 348ms\n",
      "108:\tlearn: 0.4379870\ttotal: 18.9s\tremaining: 174ms\n",
      "109:\tlearn: 0.4378382\ttotal: 19.1s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1c4c5373c8>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = CatBoostClassifier(random_state=16, \n",
    "                            iterations=110,\n",
    "                            learning_rate=0.1,\n",
    "                            depth=6,\n",
    "                            )\n",
    "\n",
    "model_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:29:58.966951Z",
     "start_time": "2020-07-25T11:29:58.951994Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_2 = model_2.predict_proba(X_val).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:46:59.605550Z",
     "start_time": "2020-07-25T10:46:59.545518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12254,  3860],\n",
       "       [    2,   121]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred_2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:47:00.540608Z",
     "start_time": "2020-07-25T10:47:00.512979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8720982915426773"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_pred_2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T10:52:05.318553Z",
     "start_time": "2020-07-25T10:52:05.298209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8292712477110563"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_comb, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:22:21.075420Z",
     "start_time": "2020-07-25T11:17:42.447474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=6, n_estimators=200, random_state=1,\n",
       "                       warm_start=True)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = RandomForestClassifier(random_state = 1, n_estimators=200, \n",
    "                                max_depth = 6, warm_start=True)\n",
    "\n",
    "model_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:29:59.552983Z",
     "start_time": "2020-07-25T11:29:58.971064Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_3 = model_3.predict_proba(X_val).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:05:18.785944Z",
     "start_time": "2020-07-25T11:05:18.768076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8474948995783651"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_pred_3, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:05:48.411810Z",
     "start_time": "2020-07-25T11:05:48.380660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12247,  3853],\n",
       "       [    9,   128]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred_3, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:41:09.224951Z",
     "start_time": "2020-07-25T11:40:57.767384Z"
    }
   },
   "outputs": [],
   "source": [
    "model_4 = KNeighborsClassifier(n_neighbors=7, weights=\"uniform\",leaf_size=20)\n",
    "model_4.fit(X_train, y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3,7,13,18],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"eucledian\",\"manhattan\"],\n",
    "    \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    \"leaf_size\": [20,30,40],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:41:21.136789Z",
     "start_time": "2020-07-25T11:41:19.056786Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_4 = model_4.predict_proba(X_val).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:41:11.527300Z",
     "start_time": "2020-07-25T11:41:11.512701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5816806795819155"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_pred_4, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# комбинация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:48:23.643465Z",
     "start_time": "2020-07-25T11:48:23.635827Z"
    }
   },
   "outputs": [],
   "source": [
    "y_comb = 0.3*y_pred_1 + 0.3*y_pred_2 + 0.3* y_pred_3 + 0.1* y_pred_4\n",
    "y_comb = np.where(y_comb<0.77, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:48:23.967174Z",
     "start_time": "2020-07-25T11:48:23.955107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87752587481518"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_comb, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T11:48:13.976061Z",
     "start_time": "2020-07-25T11:48:13.947067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12256,  3941],\n",
       "       [    0,    40]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_comb, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold на трэйн выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-23T21:34:05.287Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5,shuffle=False)\n",
    "kf.split(X_train)    \n",
    "\n",
    "accuracy_model = []\n",
    "errors = []\n",
    "\n",
    "\n",
    "\n",
    "r_s = range(1,20)\n",
    "\n",
    "for r in r_s:\n",
    "    clf = XGBClassifier(random_state=r, base_score=0.9,  eval_metric='auc')\n",
    "    for train_index, test_index in kf.split(X_train_main):\n",
    "        # Split train-test\n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        # Train the model\n",
    "        model = clf.fit(X_train_fold, y_train_fold)\n",
    "        # Append to accuracy_model the accuracy of the model\n",
    "        try:\n",
    "            score = roc_auc_score(y_test, model.predict(X_val))\n",
    "            print(f\"rand st {r}: {score}\")\n",
    "        except: \n",
    "            errors.append(\"Oops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
